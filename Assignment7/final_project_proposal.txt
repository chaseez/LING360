For my final project, I want to create one large language model for 18th-19th century English and compare sentence similarity for common phrases used back then and now, using an existing large language model found on HuggingFace.co. I want to do this because I want experience building my own large language model, have hands-on experience training it, and be able to use the vector space encodings to extrapolate conclusions. This will help me in my future plans because I plan on going into either Computation Linguistics or Natural Language Processing for Machine Translation and I know that using large language models can be an efficient way of using a preexisting encoding\decoding layer for neural networks in machine translation models. So, getting an better understanding of how large language models are trained and stored will help broaden the tools I can use for machine translation.

The training and validation data I will use consists of old British English from the 1600's-1900's. In terms of testing data, I will look up and compile a list of 10 of the most common phrases from each century.

In terms of data, I have found roughly 6 GB of free data, mostly in .xml files with hundreds of millions of tokens.I plan on taking the first two weeks to clean the data, create the training, test, and validation sets, and do research for frameworks that can create a large language model that I want. Then for the next 4 weeks, I will play around with the model and different techniques to retrieve the word embeddings and compare them to the pre-trained large language model. To ensure accuracy of my models I will gather vetted sentences from each century for the large language model to test against and get the resulting vector embeddings to see if the model is up to standard.

I will need to know how to use Google Colab, which has access to simple GPUs, the Python to read a .tmx file, clean each line of HTML tags, any excess symbols that don't add to word meanings, and the neural network frameworks necessary for generating a large language model. Another option, if I have time, is to make the large language model generative.