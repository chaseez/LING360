# -*- coding: utf-8 -*-
"""CustomLLM_Tutorial.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FRzxHo9p_itmjYbQexRJwvklHHvptOlD

Here's the link to the YouTube tutorial:
- https://www.youtube.com/watch?v=UU1WVnMk4E8
"""

"""-------------------------------------------------------------------------
This section is about PyTorch functions that are handy and will be used in creating the LLM model
"""

# Handy pytorch functions.
import torch
device = torch.device('cuda')

randint = torch.randint(-100, 100, (6,))
print(randint)
# Example output: tensor([-79, -84,  18,  19,  66,  73])

tensor = torch.tensor([[0.1,0.01,0.001],[0.2,0.02,0.002],[0.3,0.03,0.003]])
print(tensor)
# Example output: tensor([[0.1000, 0.0100, 0.0010],
                        # [0.2000, 0.0200, 0.0020],
                        # [0.3000, 0.0300, 0.0030]])

zeros = torch.zeros(2,3)
print(zeros)
# Example output: tensor([[0., 0., 0.],
                        # [0., 0., 0.]])

ones = torch.ones(3,4)
print(ones)
# Example output: tensor([[1., 1., 1., 1.],
                        # [1., 1., 1., 1.],
                        # [1., 1., 1., 1.]])

_range = torch.arange(10)
print(_range)
# Example output: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

linspace = torch.linspace(7, 25, steps=8)
print(linspace)
# Example output: tensor([ 7.0000,  9.5714, 12.1429, 14.7143, 17.2857, 19.8571, 22.4286, 25.0000])
"""
torch.linspace(start=int,end=int,steps=int)

This function allows you to generate a tensor with
tensor.long numbers from "start" to "end" in "steps"
number of linear steps
"""

logspace = torch.logspace(start=-10, end=10, steps=7)
print(logspace)
# Example output: tensor([1.0000e-10, 2.1544e-07, 4.6416e-04, 1.0000e+00, 2.1544e+03, 4.6416e+06, 1.0000e+10])
"""
torch.logspace(start=int,end=int,steps=int)

This function allows you to generate a tensor with
tensor.float numbers from "10^start" to "10^end" in "steps"
number of linear steps
"""

eye = torch.eye(6)
print(eye)
# Example output: tensor([[1., 0., 0., 0., 0., 0.],
                        # [0., 1., 0., 0., 0., 0.],
                        # [0., 0., 1., 0., 0., 0.],
                        # [0., 0., 0., 1., 0., 0.],
                        # [0., 0., 0., 0., 1., 0.],
                        # [0., 0., 0., 0., 0., 1.]])

a = torch.empty((2,3), dtype=torch.int64)
print(a)
# Example output: tensor([[  135256372657536,   135256372657536,                 1],
                        # [72340172821233664,                48,               128]])
"""
torch.empty(dimensions=Tuple, dtype=Any)
This function takes a tuple and creates a tensor of size dimensions
and fills them with random numbers of type "dtype"
"""

empty_like = torch.empty_like(a)
print(empty_like)
# Example output: tensor([[135256372657552, 135256372657552,               1],
                        # [     4294967296,              48,              80]])
"""
torch.empty_like(tensor=Tensor)
Takes a tensor and uses it as a template to generate a new
tensor, like calling tensor.empty() with exisiting parameters.0
"""
print()

import time
import numpy as np

start_time = time.time()
zeros = torch.zeros(1,1)
end_time = time.time()

elapsed_time = end_time - start_time
print(f'{elapsed_time:.10f}') # Can use (float|long|int):.(number of sig figs)(dtype) to format numbers

# Testing the CUDA GPU speed with torch compared to the CPU using numpy
torch_rand1 = torch.rand(100,100,100,100).to(device)
torch_rand2 = torch.rand(100,100,100,100).to(device)
np_rand1 = torch.rand(100,100,100,100)
np_rand2 = torch.rand(100,100,100,100)

# CUDA GPU using PyTorch
start_time = time.time()

rand = (torch_rand1 @ torch_rand2)

end_time = time.time()
elapsed_time = end_time - start_time
print(f'{elapsed_time:.10f}')
# Output: 0.0004467964

# --------------------------------------------

# CPU using numpy
start_time = time.time()

rand = np.multiply(np_rand1,np_rand2)

end_time = time.time()
elapsed_time = end_time - start_time
print(f'{elapsed_time:.10f}')
# Output: 0.2574915886

# Other handy PyTorch functions

# Define a probablilty tensor
probabilities = torch.tensor([0.1,0.9])
# 10% or 0.1 => 0, 90% or 0.9 => 1. Each probability points to the index of the probability in the tensor
# Draw 5 samples from teh multinomial distribution
samples = torch.multinomial(probabilities, num_samples=10, replacement=True)
print(samples)
# Example output: tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1])

# Returns a tensor with ones filled below the diagonal based on the tensor passed in
out = torch.tril(torch.ones(5,5))
print(out)
"""
Example output:
tensor([[1., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0.],
        [1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1.]])
"""

# Returns a tensor with ones filled above the diagonal based on the tensor passed in
out = torch.triu(torch.ones(5,5))
print(out)
"""
Example output:
tensor([[1., 1., 1., 1., 1.],
        [0., 1., 1., 1., 1.],
        [0., 0., 1., 1., 1.],
        [0., 0., 0., 1., 1.],
        [0., 0., 0., 0., 1.]])
"""

# Take a zero tensor, fill in the upper triangle with ones, except for the
# diagonal, and where the 1's will be made into -inf
# This is handy for later becase torch.exp(-inf) == 0
out = torch.zeros(5,5).masked_fill(torch.tril(torch.ones(5,5)) == 0, float('-inf'))
print(out)
"""
Example output:
tensor([[0., -inf, -inf, -inf, -inf],
        [0., 0., -inf, -inf, -inf],
        [0., 0., 0., -inf, -inf],
        [0., 0., 0., 0., -inf],
        [0., 0., 0., 0., 0.]])
"""

# Returns a tensor where each number is replaced with e^t[i,j]
out = torch.exp(out)
print(out)
"""
Example output:
tensor([[1., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0.],
        [1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1.]])
"""

# Make a tensor of 2 rows, 3 col, 4 height
input = torch.zeros(2,3,4)
# Transpose the 0th dimension with the 2nd dimension
out = input.transpose(0,2)
print(out.shape)
# Example output: torch.Size([4, 3, 2])

tensor1 = torch.tensor([1,2,3])
tensor2 = torch.tensor([4,5,6])
tensor3 = torch.tensor([7,8,9])

# Takes a list of tensors and creates a new tensor in the new order,
# increasing the dimensions by 1
stacked_tensor = torch.stack([tensor1,tensor2,tensor3])
print(stacked_tensor)

import torch.nn as nn
sample = torch.tensor([10.,10.,10.])

# Initializes the Linear class, which allows for a linear transformation
# to occur between hidden layers to apply the different weights and biases
# to each node in the neural net
linear = nn.Linear(3,3,bias=False)
print(linear(sample))

import torch.nn.functional as F

tensor1 = torch.tensor([10.0,5.0,4.0])

# Takes all the inputs, performs torch.exp() for each value.
# Then normalizes each value based on the total sum
softmax_output = F.softmax(tensor1, dim=0)
print(softmax_output)

"""-------------------------------------------------------------------------

How to do word embeddings!
"""

vocab_size = 10000
embedding_dim = 100
embedding = nn.Embedding(vocab_size, embedding_dim)

input_indices = torch.LongTensor([1,5,3,2])

embedded_output = embedding(input_indices)
print(embedded_output)

print(embedded_output.shape)

"""-------------------------------------------------------------------------"""

import torch.nn as nn

block_size = 8
batch_size = 4
max_iters = 10000
eval_interval = 2500
learning_rate = 3e-4
eval_iters = 250

with open('./drive/MyDrive/LING 360 Final Project/Cleaned Data', 'r', encoding='utf-8') as file:
  text = file.read()

chars = sorted(set(text))
vocab_size= len(chars)
print(chars)

# Encoding and decoding the text
string_to_int = { ch:i for i,ch in enumerate(chars) }
int_to_string = { i:ch for i,ch in enumerate(chars) }

encode = lambda s: [string_to_int[c] for c in s] # s for string
decode = lambda l: ''.join([int_to_string[i] for i in l]) # l for list

encoded_message = encode('hello')
decode(encoded_message)

data = torch.tensor(encode(text), dtype=torch.long)
n = int(0.8*len(data))
train_data = data[:n]
val_data = data[n:]

def get_batch(split):
  data = train_data if split == 'train' else val_data
  ix = torch.randint(len(data) - block_size, (batch_size,))
  x = torch.stack([data[i:i+block_size] for i in ix])
  y = torch.stack([data[i+1:i+block_size+1] for i in ix])
  x,y = x.to(device), y.to(device)
  return x,y

x,y = get_batch('train')

x= train_data[:block_size]
y= train_data[1:block_size + 1]

for t in range(block_size): # Sequential printing of bigrams
  context = x[:t+1]
  target = y[t]
  print(f'when input is {context} target is {target}')

class BigramLanguageModel(nn.Module):
  def __init__(self, vocab_size):
    super().__init__()

    # Initialize a probability look up table, think SMT, where the model
    # will store the probability that one letter will follow another
    # (this only applies to bigrams)
    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)

  """
  This function is SUPER important! This is really good for debugging,
  better understanding your code, allows you to optimize and find better
  functions to use, and is best practice to write your own
  """
  def forward(self, index, targets=None):
    # Normalized percentages for the bigram at that specific index
    logits = self.token_embedding_table(index)
    # B: Batch
    # T: Time Dimension
    # C: Channels (vocab size)
    if targets is None:
      loss = None
    else:
      B, T, C = logits.shape
      logits = logits.view(B*T, C)
      targets = targets.view(B*T)
      loss = F.cross_entropy(logits, targets)

    return logits, loss

  def generate(self, index, max_new_tokens):
    # index is (B,T) array of indices in the current context
    for _ in range(max_new_tokens):
      # get the prediction
      logits, loss = self.forward(index)
      # focus only on the last time step
      logits = logits[:,-1,:] # becomes (B,C)
      # apply softmax to get probabilities
      probs = F.softmax(logits, dim=-1) # (B,C)
      # sample from the distribution
      index_next = torch.multinomial(probs, num_samples=1) # (B,1)
      # append sampled index to the running sequence
      index = torch.cat((index,index_next), dim=-1) # (B,T+1)
    return index

model = BigramLanguageModel(vocab_size)
m = model.to(device)

context = torch.zeros((1,1), dtype=torch.long, device=device)
generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())
print(generated_chars)

@torch.no_grad()
def estimate_loss():
  out = {}
  model.eval()
  for split in ['train', 'eval']:
    losses = torch.zeros(eval_iters)
    for k in range(eval_iters):
      X,Y = get_batch(split)
      logits,loss = model(X,Y)
      losses[k] = loss.item()
    out[split] = losses.mean()
  model.train()
  return out

optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
# Typical training loop for models
for iter in range(max_iters):
  break

  if iter % eval_iters == 0:
    losses = estimate_loss()
    print(f'step: {iter}, train loss: {losses["train"]:.3f} eval loss: {losses["eval"]:.3f}')
  # Sample a batch of data
  xb, yb = get_batch('train')
  # print(iter)
  logits, loss = model.forward(xb, yb)
  # Make sure that the gradients don't add up over time
  # Trying to optimize current gradient
  optimizer.zero_grad(set_to_none=True)
  # set_to_none occupies a lot less space compared to int_64 of 0's
  loss.backward()
  optimizer.step()

# print(loss.item())

context = torch.zeros((1,1), dtype=torch.long, device=device)
generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())
print(generated_chars)

"""_________________________________________________

DONE CREATING A BIGRAM LANGUAGE MODEL

TRANSFORMER ARCHITECTURE

https://arxiv.org/pdf/1706.03762.pdf

We'll be using GPT (Generative Pre-trained Transformer)

This is different from the transformer architecture in the paper, but it is very similar.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import mmap
import random
import pickle

block_size = 64
batch_size = 128
max_iters = 200
learning_rate = 3e-4
eval_iters = 100
eval_interval = 500
n_embd = 384
n_layer = 8
n_head = 8
dropout = 0.2 # Drops out 20% of neurons from the net to prevent over-fitting

device = torch.device('cuda')

with open('./drive/MyDrive/LING 360 Final Project/Cleaned Data/char_set.txt', 'r', encoding='utf-8') as file:
  text = file.read()

chars = sorted(set(text))
vocab_size= len(chars)

# Encoding and decoding the text
stoi = { ch:i for i,ch in enumerate(chars) }
itos = { i:ch for i,ch in enumerate(chars) }

encode = lambda s: [stoi[c] for c in s] # s for string
decode = lambda l: ''.join([itos[i] for i in l]) # l for list

def get_random_chunk(split):
  filename = './drive/MyDrive/LING 360 Final Project/Cleaned Data/train.txt' if split == 'train' else \
              './drive/MyDrive/LING 360 Final Project/Cleaned Data/val.txt'
  with open(filename, 'rb') as f:
    with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:
      # Determine the file size and a random position to start reading
      file_size= len(mm)
      start_pos = random.randint(0, (file_size) - block_size*batch_size)

      # Seek to the random position and read the block of text
      mm.seek(start_pos)
      block = mm.read(block_size*batch_size-1)

      # Decode the block to a string, ignoring any invalid byte sequences
      decoded_block = block.decode('utf-8', errors='ignore').replace('\r','')

      # Train and test splits
      data = torch.tensor(encode(decoded_block), dtype=torch.long)
  return data

def get_batch(split):
  data = get_random_chunk(split)
  ix = torch.randint(len(data) - block_size, (batch_size,))
  x = torch.stack([data[i:i+block_size] for i in ix])
  y = torch.stack([data[i+1:i+block_size+1] for i in ix])
  x,y = x.to(device), y.to(device)
  return x,y

x,y = get_batch('train')

class Head(nn.Module):
  def __init__(self, head_size):
    super().__init__()
    self.key = nn.Linear(n_embd, head_size, bias=False)
    self.query = nn.Linear(n_embd, head_size, bias=False)
    self.value = nn.Linear(n_embd, head_size, bias=False)
    # Helps save on computation time and complexity instead of initializing each
    # instance of the class. Think of a static method for the classes
    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))

    self.dropout = nn.Dropout(dropout)

  def forward(self, x):
    # input of size (batch, time-step, channels)
    # output of size (batch, time-step, head size)
    B,T,C = x.shape
    k = self.key(x) # (B,T,hs)
    q = self.query(x) # (B,T,hs)
    # compute attention scores ("affinities")

    # q = (B, T, hs) @ k.transpose(-2,-1) = (B, hs, T) -> (B, T, T)
    wei = q @ k.transpose(-2,-1) * (k.shape[-1]**-0.5) # this last bit is 1/sqrt(len(keys))
    # k.transpose(-2,-1) swaps the 2nd to last           # And is used to scale the individual
    # row with the last row                              # head with respect to the whole context


    wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf')) # (B, T, T)
    wei = F.softmax(wei, dim=-1) # (B,T,T)
    wei = self.dropout(wei)
    # perform the weighted aggregation of the values
    v = self.value(x) # (B,T,hs)
    out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)
    return out

class MultiHeadAttention(nn.Module):
  """ Multiple heads of self-attention in parallel """

  def __init__(self, num_heads, head_size):
    super().__init__()
    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])
    self.proj = nn.Linear(head_size * num_heads, n_embd)
    self.dropout = nn.Dropout(dropout)

  def forward(self,x):
    # Take the output and concatenate it to the last dimension of the matrix (B,T)
    # (B,T,F) -> (B,T, [h1,h1,h1,h1,h2,h2,h2,h2,h3,h3,h3,h3])
    # Each h# is a feature of that head
    out = torch.cat([h(x) for h in self.heads], dim=-1)
    out = self.dropout(self.proj(out))
    return out

class FeedForward(nn.Module):
  """ A simple linear layer followed by a non-linearity """

  def __init__(self, n_embd):
    super().__init__()
    self.net = nn.Sequential(
        nn.Linear(n_embd, 4 * n_embd),
        nn.ReLU(),
        nn.Linear(4*n_embd, n_embd),

        # This will allow (dropout * 100) % of nodes to be forgotten to prevent over fitting
        nn.Dropout(dropout)
    )

  def forward(self, x):
    return self.net(x)

class Block(nn.Module):
  """ Transformer block: communication followed by computation """

  def __init__(self, n_embd, n_head):
    super().__init__()
    head_size = n_embd // n_head
    self.sa = MultiHeadAttention(n_head, head_size)
    self.ffwd = FeedForward(n_embd)
    self.ln1 = nn.LayerNorm(n_embd)
    self.ln2 = nn.LayerNorm(n_embd)

  def forward(self, x):
    y = self.sa(x)
    x = self.ln1(x + y)
    y = self.ffwd(x)
    x = self.ln2(x + y)
    return x


class GPTLanguageModel(nn.Module):
  def __init__(self, vocab_size):
    super().__init__()

    # Initialize a probability look up table, think SMT, where the model
    # will store the probability that one letter will follow another
    # (this only applies to bigrams)
    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)
    self.position_embedding_table = nn.Embedding(block_size, n_embd)
    # nn.Sequential neccessitates that each object goes to completion before moving on to the next item
    self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])
    self.ln_f = nn.LayerNorm(n_embd)
    self.lm_head = nn.Linear(n_embd, vocab_size)

    self.apply(self._init_weights)

  def _init_weights(self, module):
    """
    Helps perform initializations on the weights, which allows for better
    training and increased performances for starting training of the model
    """
    if isinstance(module, nn.Linear):
      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
      if module.bias is not None:
        torch.nn.init.zeros_(module.bias)
    elif isinstance(module, nn.Embedding):
      torch.nn.init.normal_(module.weight, mean=0.0, std=0.2)

  """
  This function is SUPER important! This is really good for debugging,
  better understanding your code, allows you to optimize and find better
  functions to use, and is best practice to write your own
  """
  def forward(self, index, targets=None):
    # Normalized percentages for the bigram at that specific index
    B, T = index.shape

    # B: Batch
    # T: Time Dimension
    # C: Channels (vocab size)

    # index and targets are both (B,T) tensor of integers
    tok_emb = self.token_embedding_table(index) # (B,T,C)
    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)
    x = tok_emb + pos_emb # (B,T,C)
    x = self.blocks(x) # (B,T,C)
    x = self.ln_f(x) # (B,T,C)
    logits = self.lm_head(x) # (B,T,vocab_size)

    if targets is None:
      loss = None
    else:
      B,T,C = logits.shape
      logits = logits.view(B*T, C)
      targets = targets.view(B*T)
      loss = F.cross_entropy(logits, targets)

    return logits, loss

  def generate(self, index, max_new_tokens):
    # index is (B,T) array of indices in the current context
    for _ in range(max_new_tokens):
      # get the prediction
      logits, loss = self.forward(index)
      # focus only on the last time step
      logits = logits[:,-1,:] # becomes (B,C)
      # apply softmax to get probabilities
      probs = F.softmax(logits, dim=-1) # (B,C)
      # sample from the distribution
      index_next = torch.multinomial(probs, num_samples=1) # (B,1)
      # append sampled index to the running sequence
      index = torch.cat((index,index_next), dim=-1) # (B,T+1)
    return index

@torch.no_grad()
def estimate_loss():
  out = {}
  model.eval()
  for split in ['train', 'eval']:
    losses = torch.zeros(eval_iters)
    for k in range(eval_iters):
      X,Y = get_batch(split)
      logits,loss = model(X,Y)
      losses[k] = loss.item()
    out[split] = losses.mean()
  model.train()
  return out

model = GPTLanguageModel(vocab_size)
print('loading model parameters...')
with open('./drive/MyDrive/LING 360 Final Project/Models/model-01.pkl', 'rb') as f:
  model = pickle.load(f)
print('model loaded')
m = model.to(device)

optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
# Typical training loop for models
for iter in range(max_iters):

  if iter % eval_iters == 0:
    losses = estimate_loss()
    print(f'step: {iter}, train loss: {losses["train"]:.3f} eval loss: {losses["eval"]:.3f}')
  # Sample a batch of data
  xb, yb = get_batch('train')
  # print(iter)
  logits, loss = model.forward(xb, yb)
  # Make sure that the gradients don't add up over time
  # Trying to optimize current gradient
  optimizer.zero_grad(set_to_none=True)
  # set_to_none occupies a lot less space compared to int_64 of 0's
  loss.backward()
  optimizer.step()

print(loss.item())
with open('./drive/MyDrive/LING 360 Final Project/Models/model-01.pkl', 'wb') as f:
  pickle.dump(model, f)
print('model saved')

context = torch.zeros((1,1), dtype=torch.long, device=device)
generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())
print(generated_chars)
"""
_____________________________________
Activation functions in pytorch.nn and pytorch.nn.functional

Typically used for normailizing inputs
"""

# nn.ReLU takes a tensor
# For every value below 0, it transforms it to 0
# and leaves any value greater than 0 as the same

x = torch.tensor([-0.05], dtype=torch.float32)
y = nn.ReLU(x)
print(y)

# nn.functional.sigmoid
# The actual function is 1 / (1 + e^(-x))
y = F.sigmoid(x)
print(y)

"""Sigmoid function returns values between 0 and 1; this is different from the softmax function which takes all the values and returns a tensor with the values normalized, meaning they all add up to 1. Can be helpful for probability values."""

# nn.fucntional.tanh
# The actual function is (e^x - e^(-x))/(e^x + e^(-x))
y = F.tanh(x)
print(y)

"""Tanh function return values between -1 and 1. This is very similar to sigmoid in that it normalizes data between a specific range and they both have an S shaped graph."""

